From 012124f4367e1ba7fb9a1013286e3e1acfaee2c8 Mon Sep 17 00:00:00 2001
From: Karthik Manamcheri <karthik@cloudera.com>
Date: Wed, 25 Jul 2018 17:02:04 -0500
Subject: [PATCH 1415/1431] CDH-70340: HIVE-19782: Flash out
 TestObjectStore.testDirectSQLDropParitionsCleanup
 (Peter Vary, reviewed by Vihang Karajgaonkar)

(cherry-picked from commit 8f0973b28eabbebf1563e1bfeac3609359d05688)

This change also includes a bug fix picked from HIVE-16147. The fix
is required to run the new tests added. The partial code-chunk picked
is part of commit e86461fb8cc7dc6f4a7cc1ecd43d037c69c2777a

==C5_FEATURE_IMPALA_METADATA==

Change-Id: I833e031604e4197c0212fe396e4667d933726a5c
---
 .../apache/hadoop/hive/metastore/ObjectStore.java  |    7 +-
 .../client/builder/HiveObjectRefBuilder.java       |   69 ++++++++++
 .../hadoop/hive/metastore/TestObjectStore.java     |  132 +++++++++++++++++---
 3 files changed, 188 insertions(+), 20 deletions(-)
 create mode 100644 metastore/src/java/org/apache/hadoop/hive/metastore/client/builder/HiveObjectRefBuilder.java

diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
index ea49647..19fb3b5 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
@@ -6872,7 +6872,12 @@ protected String describeResult() {
     try {
       openTransaction();
       // We are not going to verify SD for each partition. Just verify for the table.
-      validateTableCols(table, colNames);
+      // ToDo: we need verify the partition column instead
+      try {
+        validateTableCols(table, colNames);
+      } catch (MetaException me) {
+        LOG.warn("The table does not have the same column definition as its partition.");
+      }
       Query query = queryWrapper.query = pm.newQuery(MPartitionColumnStatistics.class);
       String paramStr = "java.lang.String t1, java.lang.String t2";
       String filter = "tableName == t1 && dbName == t2 && (";
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/client/builder/HiveObjectRefBuilder.java b/metastore/src/java/org/apache/hadoop/hive/metastore/client/builder/HiveObjectRefBuilder.java
new file mode 100644
index 0000000..ab0ea82
--- /dev/null
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/client/builder/HiveObjectRefBuilder.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.metastore.client.builder;
+
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.HiveObjectRef;
+import org.apache.hadoop.hive.metastore.api.HiveObjectType;
+import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.metastore.api.Table;
+
+import java.util.Collections;
+import java.util.List;
+
+/**
+ * A builder for {@link HiveObjectRef}.  Unlike most builders (which allow a gradual building up
+ * of the values) this gives a number of methods that take the object to be referenced and then
+ * build the appropriate reference.  This is intended primarily for use with
+ * {@link HiveObjectPrivilegeBuilder}
+ */
+public class HiveObjectRefBuilder {
+  private HiveObjectType objectType;
+  private String dbName, objectName, columnName;
+  private List<String> partValues;
+
+  public HiveObjectRef buildGlobalReference() {
+    return new HiveObjectRef(HiveObjectType.GLOBAL, null, null, Collections.<String>emptyList(), null);
+  }
+
+  public HiveObjectRef buildDatabaseReference(Database db) {
+    return new
+        HiveObjectRef(HiveObjectType.DATABASE, db.getName(), null, Collections.<String>emptyList(), null);
+  }
+
+  public HiveObjectRef buildTableReference(Table table) {
+    return new HiveObjectRef(HiveObjectType.TABLE, table.getDbName(), table.getTableName(),
+        Collections.<String>emptyList(), null);
+  }
+
+  public HiveObjectRef buildPartitionReference(Partition part) {
+    return new HiveObjectRef(HiveObjectType.PARTITION, part.getDbName(), part.getTableName(),
+        part.getValues(), null);
+  }
+
+  public HiveObjectRef buildColumnReference(Table table, String columnName) {
+    return new HiveObjectRef(HiveObjectType.COLUMN, table.getDbName(), table.getTableName(),
+        Collections.<String>emptyList(), columnName);
+  }
+
+  public HiveObjectRef buildPartitionColumnReference(Table table, String columnName,
+                                                     List<String> partValues) {
+    return new HiveObjectRef(HiveObjectType.COLUMN, table.getDbName(), table.getTableName(),
+        partValues, columnName);
+  }
+}
diff --git a/metastore/src/test/org/apache/hadoop/hive/metastore/TestObjectStore.java b/metastore/src/test/org/apache/hadoop/hive/metastore/TestObjectStore.java
index 04813d4..4cd3009 100644
--- a/metastore/src/test/org/apache/hadoop/hive/metastore/TestObjectStore.java
+++ b/metastore/src/test/org/apache/hadoop/hive/metastore/TestObjectStore.java
@@ -17,43 +17,43 @@
  */
 package org.apache.hadoop.hive.metastore;
 
-import java.sql.*;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.Callable;
-import java.util.concurrent.CyclicBarrier;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Future;
-
+import org.apache.hadoop.hive.common.metrics.MetricsTestUtils;
 import org.apache.hadoop.hive.common.metrics.common.MetricsConstant;
 import org.apache.hadoop.hive.common.metrics.common.MetricsFactory;
 import org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics;
 import org.apache.hadoop.hive.common.metrics.metrics2.MetricsReporting;
-import org.apache.hadoop.hive.common.metrics.MetricsTestUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;
+import org.apache.hadoop.hive.metastore.api.ColumnStatistics;
+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;
+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc;
+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;
 import org.apache.hadoop.hive.metastore.api.CurrentNotificationEventId;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Function;
+import org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege;
+import org.apache.hadoop.hive.metastore.api.HiveObjectRef;
 import org.apache.hadoop.hive.metastore.api.InvalidInputException;
 import org.apache.hadoop.hive.metastore.api.InvalidObjectException;
 import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
 import org.apache.hadoop.hive.metastore.api.NotificationEvent;
 import org.apache.hadoop.hive.metastore.api.NotificationEventRequest;
 import org.apache.hadoop.hive.metastore.api.NotificationEventResponse;
-import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
 import org.apache.hadoop.hive.metastore.api.Partition;
 import org.apache.hadoop.hive.metastore.api.PrincipalType;
+import org.apache.hadoop.hive.metastore.api.PrivilegeBag;
+import org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo;
 import org.apache.hadoop.hive.metastore.api.Role;
 import org.apache.hadoop.hive.metastore.api.SerDeInfo;
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
 import org.apache.hadoop.hive.metastore.api.Table;
 import org.apache.hadoop.hive.metastore.client.builder.DatabaseBuilder;
+import org.apache.hadoop.hive.metastore.client.builder.HiveObjectPrivilegeBuilder;
+import org.apache.hadoop.hive.metastore.client.builder.HiveObjectRefBuilder;
 import org.apache.hadoop.hive.metastore.client.builder.PartitionBuilder;
+import org.apache.hadoop.hive.metastore.client.builder.PrivilegeGrantInfoBuilder;
 import org.apache.hadoop.hive.metastore.client.builder.TableBuilder;
 import org.apache.hadoop.hive.metastore.messaging.EventMessage;
 import org.apache.hadoop.hive.serde.serdeConstants;
@@ -65,6 +65,22 @@
 import org.junit.Ignore;
 import org.junit.Test;
 
+import java.sql.Connection;
+import java.sql.DriverManager;
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.sql.Statement;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.Callable;
+import java.util.concurrent.CyclicBarrier;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Future;
+
 import static java.util.concurrent.Executors.newFixedThreadPool;
 
 public class TestObjectStore {
@@ -408,7 +424,7 @@ public void testPartitionOps() throws MetaException, InvalidObjectException, NoS
   @Test
   public void testDirectSQLDropPartitionsCacheInSession()
       throws TException {
-    createPartitionedTable();
+    createPartitionedTable(false, false);
     // query the partitions with JDO
     Deadline.startTimer("getPartition");
     List<Partition> partitions = objectStore.getPartitionsInternal(DB1, TABLE1,
@@ -438,7 +454,7 @@ public void testDirectSQLDropPartitionsCacheCrossSession()
     ObjectStore objectStore2 = new ObjectStore();
     objectStore2.setConf(objectStore.getConf());
 
-    createPartitionedTable();
+    createPartitionedTable(false, false);
     // query the partitions with JDO in the 1st session
     Deadline.startTimer("getPartition");
     List<Partition> partitions = objectStore.getPartitionsInternal(DB1, TABLE1,
@@ -469,10 +485,25 @@ public void testDirectSQLDropPartitionsCacheCrossSession()
    * @throws SQLException
    */
   @Test
-  public void testDirectSQLDropParitionsCleanup() throws TException,
+  public void testDirectSQLDropPartitionsCleanup() throws TException,
       SQLException {
 
-    createPartitionedTable();
+    createPartitionedTable(true, true);
+
+    // Check, that every table in the expected state before the drop
+    checkBackendTableSize("PARTITIONS", 3);
+    checkBackendTableSize("PART_PRIVS", 3);
+    checkBackendTableSize("PART_COL_PRIVS", 3);
+    checkBackendTableSize("PART_COL_STATS", 3);
+    checkBackendTableSize("PARTITION_PARAMS", 0);
+    checkBackendTableSize("PARTITION_KEY_VALS", 3);
+    checkBackendTableSize("SD_PARAMS", 3);
+    checkBackendTableSize("BUCKETING_COLS", 3);
+    checkBackendTableSize("SKEWED_COL_NAMES", 3);
+    checkBackendTableSize("SDS", 4); // Table has an SDS
+    checkBackendTableSize("SORT_COLS", 3);
+    checkBackendTableSize("SERDE_PARAMS", 3);
+    checkBackendTableSize("SERDES", 4); // Table has a serde
 
     // drop the partitions
     Deadline.startTimer("dropPartitions");
@@ -497,10 +528,12 @@ public void testDirectSQLDropParitionsCleanup() throws TException,
 
   /**
    * Creates DB1 database, TABLE1 table with 3 partitions
+   * @param withPrivileges Should we create privileges as well
+   * @param withStatistics Should we create statitics as well
    * @throws MetaException
    * @throws InvalidObjectException
    */
-  private void createPartitionedTable() throws TException {
+  private void createPartitionedTable(boolean withPrivileges, boolean withStatistics) throws TException {
     Database db1 = new DatabaseBuilder()
         .setName(DB1)
         .setDescription("description")
@@ -514,16 +547,77 @@ private void createPartitionedTable() throws TException {
             .addCol("test_col1", "int")
             .addCol("test_col2", "int")
             .addPartCol("test_part_col", "int")
+            .addCol("test_bucket_col", "int", "test bucket col comment")
+            .addCol("test_skewed_col", "int", "test skewed col comment")
+            .addCol("test_sort_col", "int", "test sort col comment")
             .build();
     objectStore.createTable(tbl1);
 
+    PrivilegeBag privilegeBag = new PrivilegeBag();
+
     // Create partitions for the partitioned table
     for(int i=0; i < 3; i++) {
       Partition part = new PartitionBuilder()
           .fromTable(tbl1)
           .addValue("a" + i)
+          .addSerdeParam("serdeParam", "serdeParamValue")
+          .addStorageDescriptorParam("sdParam", "sdParamValue")
+          .addBucketCol("test_bucket_col")
+          .addSkewedColName("test_skewed_col")
+          .addSortCol("test_sort_col", 1)
           .build();
       objectStore.addPartition(part);
+
+      if (withPrivileges) {
+        HiveObjectRef partitionReference = new HiveObjectRefBuilder().buildPartitionReference(part);
+        HiveObjectRef partitionColumnReference = new HiveObjectRefBuilder()
+            .buildPartitionColumnReference(tbl1, "test_part_col", part.getValues());
+        PrivilegeGrantInfo privilegeGrantInfo = new PrivilegeGrantInfoBuilder()
+            .setPrivilege("a")
+            .build();
+        HiveObjectPrivilege partitionPriv = new HiveObjectPrivilegeBuilder()
+            .setHiveObjectRef(partitionReference)
+            .setPrincipleName("a")
+            .setPrincipalType(PrincipalType.USER)
+            .setGrantInfo(privilegeGrantInfo)
+            .build();
+        privilegeBag.addToPrivileges(partitionPriv);
+        HiveObjectPrivilege partitionColPriv = new HiveObjectPrivilegeBuilder()
+            .setHiveObjectRef(partitionColumnReference)
+            .setPrincipleName("a")
+            .setPrincipalType(PrincipalType.USER)
+            .setGrantInfo(privilegeGrantInfo)
+            .build();
+        privilegeBag.addToPrivileges(partitionColPriv);
+      }
+
+      if (withStatistics) {
+        ColumnStatistics stats = new ColumnStatistics();
+        ColumnStatisticsDesc desc = new ColumnStatisticsDesc();
+        desc.setDbName(tbl1.getDbName());
+        desc.setTableName(tbl1.getTableName());
+        desc.setPartName("test_part_col=a" + i);
+        stats.setStatsDesc(desc);
+
+        List<ColumnStatisticsObj> statsObjList = new ArrayList<>(1);
+        stats.setStatsObj(statsObjList);
+
+        ColumnStatisticsData data = new ColumnStatisticsData();
+        BooleanColumnStatsData boolStats = new BooleanColumnStatsData();
+        boolStats.setNumTrues(0);
+        boolStats.setNumFalses(0);
+        boolStats.setNumNulls(0);
+        data.setBooleanStats(boolStats);
+
+        ColumnStatisticsObj partStats = new ColumnStatisticsObj("test_part_col", "int", data);
+        statsObjList.add(partStats);
+
+        objectStore.updatePartitionColumnStatistics(stats, part.getValues());
+      }
+    }
+
+    if (withPrivileges) {
+      objectStore.grantPrivileges(privilegeBag);
     }
   }
 
-- 
1.7.9.5

