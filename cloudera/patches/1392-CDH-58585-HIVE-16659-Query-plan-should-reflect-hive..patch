From 6ba66a7c6e4d564392ea8ac5f5e662ece4c86aa9 Mon Sep 17 00:00:00 2001
From: Bing Li <sarah.libing@gmail.com>
Date: Wed, 5 Jul 2017 15:46:43 +0800
Subject: [PATCH 1392/1431] CDH-58585: HIVE-16659: Query plan should reflect
 hive.spark.use.groupby.shuffle (Bing Li reviewed
 by Rui)

(cherry picked from commit 5726f84c8197bbbb77d077f9c70953ce9796558b)
(cherry picked from commit a5dbdc867d2d880ab39b3d9485c18534d48553fc)
(cherry picked from commit 67e15a9da212a86b271d3df287b3f7bed4dcd001)

Conflicts:
	itests/src/test/resources/testconfiguration.properties

Change-Id: Ic7fe780ca4914e7743070fcb8f2a925e5a637773
---
 .../test/resources/testconfiguration.properties    |    3 +-
 .../hadoop/hive/ql/parse/spark/GenSparkUtils.java  |   21 +++-
 .../hadoop/hive/ql/parse/spark/GenSparkWork.java   |    2 +-
 .../clientpositive/spark_explain_groupbyshuffle.q  |    8 ++
 .../spark/spark_explain_groupbyshuffle.q.out       |  116 ++++++++++++++++++++
 5 files changed, 142 insertions(+), 8 deletions(-)
 create mode 100644 ql/src/test/queries/clientpositive/spark_explain_groupbyshuffle.q
 create mode 100644 ql/src/test/results/clientpositive/spark/spark_explain_groupbyshuffle.q.out

diff --git a/itests/src/test/resources/testconfiguration.properties b/itests/src/test/resources/testconfiguration.properties
index b5a76a5..525834d 100644
--- a/itests/src/test/resources/testconfiguration.properties
+++ b/itests/src/test/resources/testconfiguration.properties
@@ -936,7 +936,8 @@ spark.only.query.files=spark_dynamic_partition_pruning.q,\
   spark_dynamic_partition_pruning_mapjoin_only.q,\
   spark_constprog_dpp.q,\
   spark_dynamic_partition_pruning_recursive_mapjoin.q,\
-  spark_vectorized_dynamic_partition_pruning.q
+  spark_vectorized_dynamic_partition_pruning.q,\
+  spark_explain_groupbyshuffle.q
 
 miniSparkOnYarn.query.files=auto_sortmerge_join_16.q,\
   bucket4.q,\
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkUtils.java
index 335daf0..063c3dc 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkUtils.java
@@ -117,8 +117,8 @@ public ReduceWork createReduceWork(GenSparkProcContext context, Operator<?> root
     setupReduceSink(context, reduceWork, reduceSink);
 
     sparkWork.add(reduceWork);
-
-    SparkEdgeProperty edgeProp = getEdgeProperty(reduceSink, reduceWork);
+    
+    SparkEdgeProperty edgeProp = getEdgeProperty(context.conf, reduceSink, reduceWork);
 
     sparkWork.connect(context.preceedingWork, reduceWork, edgeProp);
 
@@ -434,8 +434,9 @@ public void processPartitionPruningSink(GenSparkProcContext context,
     keys.add(desc.getPartKey());
   }
 
-  public static SparkEdgeProperty getEdgeProperty(ReduceSinkOperator reduceSink,
+  public static SparkEdgeProperty getEdgeProperty(HiveConf conf, ReduceSinkOperator reduceSink,
       ReduceWork reduceWork) throws SemanticException {
+    boolean useSparkGroupBy = conf.getBoolVar(HiveConf.ConfVars.SPARK_USE_GROUPBY_SHUFFLE);
     SparkEdgeProperty edgeProperty = new SparkEdgeProperty(SparkEdgeProperty.SHUFFLE_NONE);
     edgeProperty.setNumPartitions(reduceWork.getNumReduceTasks());
     String sortOrder = Strings.nullToEmpty(reduceSink.getConf().getOrder()).trim();
@@ -444,7 +445,10 @@ public static SparkEdgeProperty getEdgeProperty(ReduceSinkOperator reduceSink,
       edgeProperty.setShuffleGroup();
       // test if the group by needs partition level sort, if so, use the MR style shuffle
       // SHUFFLE_SORT shouldn't be used for this purpose, see HIVE-8542
-      if (!sortOrder.isEmpty() && groupByNeedParLevelOrder(reduceSink)) {
+      if (!useSparkGroupBy || (!sortOrder.isEmpty() && groupByNeedParLevelOrder(reduceSink))) {
+        if (!useSparkGroupBy) {
+          LOG.info("hive.spark.use.groupby.shuffle is off. Use repartition shuffle instead.");
+        }
         edgeProperty.setMRShuffle();
       }
     }
@@ -477,12 +481,17 @@ public static SparkEdgeProperty getEdgeProperty(ReduceSinkOperator reduceSink,
       }
     }
 
-    // set to groupby-shuffle if it's still NONE
     // simple distribute-by goes here
     if (edgeProperty.isShuffleNone()) {
-      edgeProperty.setShuffleGroup();
+      if (!useSparkGroupBy) {
+        LOG.info("hive.spark.use.groupby.shuffle is off. Use repartition shuffle instead.");
+        edgeProperty.setMRShuffle();
+      } else {
+        edgeProperty.setShuffleGroup();
+      }
     }
 
+
     return edgeProperty;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkWork.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkWork.java
index 3dd6d92..fcc36fb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkWork.java
@@ -217,7 +217,7 @@ public Object process(Node nd, Stack<Node> stack,
           "AssertionError: expected operator to be a ReduceSinkOperator, but was "
           + parent.getClass().getName());
         ReduceSinkOperator rsOp = (ReduceSinkOperator) parent;
-        SparkEdgeProperty edgeProp = GenSparkUtils.getEdgeProperty(rsOp, reduceWork);
+        SparkEdgeProperty edgeProp = GenSparkUtils.getEdgeProperty(context.conf, rsOp, reduceWork);
 
         rsOp.getConf().setOutputName(reduceWork.getName());
         GenMapRedUtils.setKeyAndValueDesc(reduceWork, rsOp);
diff --git a/ql/src/test/queries/clientpositive/spark_explain_groupbyshuffle.q b/ql/src/test/queries/clientpositive/spark_explain_groupbyshuffle.q
new file mode 100644
index 0000000..cd2cba1
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/spark_explain_groupbyshuffle.q
@@ -0,0 +1,8 @@
+set hive.spark.use.groupby.shuffle=true;
+
+explain select key, count(value) from src group by key;
+
+
+set hive.spark.use.groupby.shuffle=false;
+
+explain select key, count(value) from src group by key;
diff --git a/ql/src/test/results/clientpositive/spark/spark_explain_groupbyshuffle.q.out b/ql/src/test/results/clientpositive/spark/spark_explain_groupbyshuffle.q.out
new file mode 100644
index 0000000..683676a
--- /dev/null
+++ b/ql/src/test/results/clientpositive/spark/spark_explain_groupbyshuffle.q.out
@@ -0,0 +1,116 @@
+PREHOOK: query: explain select key, count(value) from src group by key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain select key, count(value) from src group by key
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Spark
+      Edges:
+        Reducer 2 <- Map 1 (GROUP, 4)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: src
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: key (type: string), value (type: string)
+                    outputColumnNames: key, value
+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      aggregations: count(value)
+                      keys: key (type: string)
+                      mode: hash
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: string)
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: string)
+                        Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col1 (type: bigint)
+        Reducer 2 
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: count(VALUE._col0)
+                keys: KEY._col0 (type: string)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1
+                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: explain select key, count(value) from src group by key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain select key, count(value) from src group by key
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Spark
+      Edges:
+        Reducer 2 <- Map 1 (GROUP PARTITION-LEVEL SORT, 4)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: src
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: key (type: string), value (type: string)
+                    outputColumnNames: key, value
+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      aggregations: count(value)
+                      keys: key (type: string)
+                      mode: hash
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: string)
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: string)
+                        Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col1 (type: bigint)
+        Reducer 2 
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: count(VALUE._col0)
+                keys: KEY._col0 (type: string)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1
+                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
-- 
1.7.9.5

