From 1baefb35d12efef832603785b9bb3e46986431c4 Mon Sep 17 00:00:00 2001
From: Peter Vary <pvary@cloudera.com>
Date: Thu, 19 Jul 2018 11:05:07 +0200
Subject: [PATCH 1405/1431] CDH-70877: HIVE-20183: Inserting from bucketed
 table can cause data loss, if the source table
 contains empty bucket (Peter Vary, reviewed by
 Naveen Gangam)

==C5_APPROVED_BUGFIX==

(cherry picked from commit 2cfe57faa54b5601d21b2b5526335492910cc119)

Change-Id: I1e37d60f8f3f0007d79f358d8136e3785b984262
---
 .../test/resources/testconfiguration.properties    |    1 +
 .../hadoop/hive/ql/exec/TableScanOperator.java     |    7 +++
 ql/src/test/queries/clientpositive/bucket7.q       |   13 +++++
 ql/src/test/results/clientpositive/bucket7.q.out   |   54 ++++++++++++++++++++
 .../results/clientpositive/spark/bucket7.q.out     |   54 ++++++++++++++++++++
 5 files changed, 129 insertions(+)
 create mode 100644 ql/src/test/queries/clientpositive/bucket7.q
 create mode 100644 ql/src/test/results/clientpositive/bucket7.q.out
 create mode 100644 ql/src/test/results/clientpositive/spark/bucket7.q.out

diff --git a/itests/src/test/resources/testconfiguration.properties b/itests/src/test/resources/testconfiguration.properties
index 525834d..81741ec 100644
--- a/itests/src/test/resources/testconfiguration.properties
+++ b/itests/src/test/resources/testconfiguration.properties
@@ -429,6 +429,7 @@ spark.query.files=add_part_multiple.q, \
   bucket2.q, \
   bucket3.q, \
   bucket4.q, \
+  bucket7.q, \
   bucket_map_join_1.q, \
   bucket_map_join_2.q, \
   bucket_map_join_spark1.q, \
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
index e242384..1dbb831 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
@@ -102,6 +102,10 @@ public void processOp(Object row, int tag) throws HiveException {
   @Override
   public void cleanUpInputFileChangedOp() throws HiveException {
     inputFileChanged = true;
+    updateFileId();
+  }
+
+  private void updateFileId() {
     // If the file name to bucket number mapping is maintained, store the bucket number
     // in the execution context. This is needed for the following scenario:
     // insert overwrite table T1 select * from T2;
@@ -224,6 +228,9 @@ protected void initializeOp(Configuration hconf) throws HiveException {
 
   @Override
   public void closeOp(boolean abort) throws HiveException {
+    if (getExecContext() != null && getExecContext().getFileId() == null) {
+      updateFileId();
+    }
     if (conf != null) {
       if (conf.isGatherStats() && stats.size() != 0) {
         publishStats();
diff --git a/ql/src/test/queries/clientpositive/bucket7.q b/ql/src/test/queries/clientpositive/bucket7.q
new file mode 100644
index 0000000..e77dd98
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/bucket7.q
@@ -0,0 +1,13 @@
+--! qt:dataset:
+-- SORT_QUERY_RESULTS
+set hive.enforce.bucketing=true;
+set hive.enforce.sorting=true;
+set hive.optimize.bucketingsorting=true;
+
+create table bucket1 (id int, val string) clustered by (id) sorted by (id ASC) INTO 4 BUCKETS;
+insert into bucket1 values (1, 'abc'), (3, 'abc');
+select * from bucket1;
+
+create table bucket2 like bucket1;
+insert overwrite table bucket2 select * from bucket1;
+select * from bucket2;
diff --git a/ql/src/test/results/clientpositive/bucket7.q.out b/ql/src/test/results/clientpositive/bucket7.q.out
new file mode 100644
index 0000000..3e87229
--- /dev/null
+++ b/ql/src/test/results/clientpositive/bucket7.q.out
@@ -0,0 +1,54 @@
+PREHOOK: query: create table bucket1 (id int, val string) clustered by (id) sorted by (id ASC) INTO 4 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@bucket1
+POSTHOOK: query: create table bucket1 (id int, val string) clustered by (id) sorted by (id ASC) INTO 4 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@bucket1
+PREHOOK: query: insert into bucket1 values (1, 'abc'), (3, 'abc')
+PREHOOK: type: QUERY
+PREHOOK: Output: default@bucket1
+POSTHOOK: query: insert into bucket1 values (1, 'abc'), (3, 'abc')
+POSTHOOK: type: QUERY
+POSTHOOK: Output: default@bucket1
+POSTHOOK: Lineage: bucket1.id EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]
+POSTHOOK: Lineage: bucket1.val SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]
+PREHOOK: query: select * from bucket1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@bucket1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from bucket1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@bucket1
+#### A masked pattern was here ####
+1	abc
+3	abc
+PREHOOK: query: create table bucket2 like bucket1
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@bucket2
+POSTHOOK: query: create table bucket2 like bucket1
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@bucket2
+PREHOOK: query: insert overwrite table bucket2 select * from bucket1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@bucket1
+PREHOOK: Output: default@bucket2
+POSTHOOK: query: insert overwrite table bucket2 select * from bucket1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@bucket1
+POSTHOOK: Output: default@bucket2
+POSTHOOK: Lineage: bucket2.id SIMPLE [(bucket1)bucket1.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: bucket2.val SIMPLE [(bucket1)bucket1.FieldSchema(name:val, type:string, comment:null), ]
+PREHOOK: query: select * from bucket2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@bucket2
+#### A masked pattern was here ####
+POSTHOOK: query: select * from bucket2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@bucket2
+#### A masked pattern was here ####
+1	abc
+3	abc
diff --git a/ql/src/test/results/clientpositive/spark/bucket7.q.out b/ql/src/test/results/clientpositive/spark/bucket7.q.out
new file mode 100644
index 0000000..3e87229
--- /dev/null
+++ b/ql/src/test/results/clientpositive/spark/bucket7.q.out
@@ -0,0 +1,54 @@
+PREHOOK: query: create table bucket1 (id int, val string) clustered by (id) sorted by (id ASC) INTO 4 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@bucket1
+POSTHOOK: query: create table bucket1 (id int, val string) clustered by (id) sorted by (id ASC) INTO 4 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@bucket1
+PREHOOK: query: insert into bucket1 values (1, 'abc'), (3, 'abc')
+PREHOOK: type: QUERY
+PREHOOK: Output: default@bucket1
+POSTHOOK: query: insert into bucket1 values (1, 'abc'), (3, 'abc')
+POSTHOOK: type: QUERY
+POSTHOOK: Output: default@bucket1
+POSTHOOK: Lineage: bucket1.id EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]
+POSTHOOK: Lineage: bucket1.val SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]
+PREHOOK: query: select * from bucket1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@bucket1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from bucket1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@bucket1
+#### A masked pattern was here ####
+1	abc
+3	abc
+PREHOOK: query: create table bucket2 like bucket1
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@bucket2
+POSTHOOK: query: create table bucket2 like bucket1
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@bucket2
+PREHOOK: query: insert overwrite table bucket2 select * from bucket1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@bucket1
+PREHOOK: Output: default@bucket2
+POSTHOOK: query: insert overwrite table bucket2 select * from bucket1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@bucket1
+POSTHOOK: Output: default@bucket2
+POSTHOOK: Lineage: bucket2.id SIMPLE [(bucket1)bucket1.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: bucket2.val SIMPLE [(bucket1)bucket1.FieldSchema(name:val, type:string, comment:null), ]
+PREHOOK: query: select * from bucket2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@bucket2
+#### A masked pattern was here ####
+POSTHOOK: query: select * from bucket2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@bucket2
+#### A masked pattern was here ####
+1	abc
+3	abc
-- 
1.7.9.5

