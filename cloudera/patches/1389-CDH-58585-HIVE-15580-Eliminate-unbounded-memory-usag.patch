From 8f442eef737c01209bebc43f9d97f3ad58c2a541 Mon Sep 17 00:00:00 2001
From: Xuefu Zhang <xuefu@uber.com>
Date: Fri, 20 Jan 2017 12:56:49 -0800
Subject: [PATCH 1389/1431] CDH-58585: HIVE-15580: Eliminate unbounded memory
 usage for orderBy and groupBy in Hive on Spark
 (reviewed by Chao Sun)

(cherry picked from commit 811b3e39ed569232c4f138c1287109ef8ebce132)
(cherry picked from commit 4343a5fd124b6b1a2094352c40f08a5150fe61e7)
(cherry picked from commit 70960f5158f5fcbd3b6d5eef19791aca87c0a8f8)

Conflicts:
	ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java
	ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ReduceTran.java
	ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SortByShuffler.java
	ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkReduceRecordHandler.java
	ql/src/test/queries/clientpositive/union_top_level.q
	ql/src/test/results/clientpositive/spark/union_remove_25.q.out
	ql/src/test/results/clientpositive/spark/union_top_level.q.out
	ql/src/test/results/clientpositive/spark/vector_outer_join5.q.out

Change-Id: Iec07b1c3c43b31ca22e50183f4489773f64ab2f8
---
 .../hadoop/hive/ql/exec/spark/GroupByShuffler.java |   10 +-
 .../hive/ql/exec/spark/HiveReduceFunction.java     |    4 +-
 .../exec/spark/HiveReduceFunctionResultList.java   |    8 +-
 .../hadoop/hive/ql/exec/spark/ReduceTran.java      |    4 +-
 .../hadoop/hive/ql/exec/spark/ShuffleTran.java     |    6 +-
 .../hadoop/hive/ql/exec/spark/SortByShuffler.java  |   73 +------
 .../hive/ql/exec/spark/SparkPlanGenerator.java     |    1 -
 .../ql/exec/spark/SparkReduceRecordHandler.java    |   26 ++-
 .../hadoop/hive/ql/exec/spark/SparkShuffler.java   |    2 +-
 .../test/queries/clientpositive/union_top_level.q  |    8 +-
 .../spark/lateral_view_explode2.q.out              |    2 +-
 .../clientpositive/spark/union_remove_25.q.out     |    2 +-
 .../clientpositive/spark/union_top_level.q.out     |  208 ++++++++++----------
 .../results/clientpositive/union_top_level.q.out   |   68 +++----
 14 files changed, 184 insertions(+), 238 deletions(-)

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/GroupByShuffler.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/GroupByShuffler.java
index e128dd2..8267515 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/GroupByShuffler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/GroupByShuffler.java
@@ -20,21 +20,23 @@
 
 import org.apache.hadoop.hive.ql.io.HiveKey;
 import org.apache.hadoop.io.BytesWritable;
+import org.apache.spark.HashPartitioner;
 import org.apache.spark.api.java.JavaPairRDD;
 
 public class GroupByShuffler implements SparkShuffler {
 
   @Override
-  public JavaPairRDD<HiveKey, Iterable<BytesWritable>> shuffle(
+  public JavaPairRDD<HiveKey, BytesWritable> shuffle(
       JavaPairRDD<HiveKey, BytesWritable> input, int numPartitions) {
-    if (numPartitions > 0) {
-      return input.groupByKey(numPartitions);
+    if (numPartitions < 0) {
+      numPartitions = 1;
     }
-    return input.groupByKey();
+    return input.repartitionAndSortWithinPartitions(new HashPartitioner(numPartitions));
   }
 
   @Override
   public String getName() {
     return "GroupBy";
   }
+
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java
index f6595f1..5348299 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java
@@ -26,7 +26,7 @@
 import scala.Tuple2;
 
 public class HiveReduceFunction extends HivePairFlatMapFunction<
-  Iterator<Tuple2<HiveKey, Iterable<BytesWritable>>>, HiveKey, BytesWritable> {
+  Iterator<Tuple2<HiveKey, BytesWritable>>, HiveKey, BytesWritable> {
 
   private static final long serialVersionUID = 1L;
 
@@ -37,7 +37,7 @@ public HiveReduceFunction(byte[] buffer, SparkReporter sparkReporter) {
   @SuppressWarnings("unchecked")
   @Override
   public Iterable<Tuple2<HiveKey, BytesWritable>>
-  call(Iterator<Tuple2<HiveKey, Iterable<BytesWritable>>> it) throws Exception {
+  call(Iterator<Tuple2<HiveKey, BytesWritable>> it) throws Exception {
     initJobConf();
 
     SparkReduceRecordHandler reducerRecordhandler = new SparkReduceRecordHandler();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunctionResultList.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunctionResultList.java
index d57cac4..8708819 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunctionResultList.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunctionResultList.java
@@ -26,7 +26,7 @@
 import scala.Tuple2;
 
 public class HiveReduceFunctionResultList extends
-    HiveBaseFunctionResultList<Tuple2<HiveKey, Iterable<BytesWritable>>> {
+    HiveBaseFunctionResultList<Tuple2<HiveKey, BytesWritable>> {
   private static final long serialVersionUID = 1L;
   private final SparkReduceRecordHandler reduceRecordHandler;
 
@@ -37,16 +37,16 @@
    * @param reducer Initialized {@link org.apache.hadoop.hive.ql.exec.mr.ExecReducer} instance.
    */
   public HiveReduceFunctionResultList(
-      Iterator<Tuple2<HiveKey, Iterable<BytesWritable>>> inputIterator,
+      Iterator<Tuple2<HiveKey, BytesWritable>> inputIterator,
       SparkReduceRecordHandler reducer) {
     super(inputIterator);
     this.reduceRecordHandler = reducer;
   }
 
   @Override
-  protected void processNextRecord(Tuple2<HiveKey, Iterable<BytesWritable>> inputRecord)
+  protected void processNextRecord(Tuple2<HiveKey, BytesWritable> inputRecord)
       throws IOException {
-    reduceRecordHandler.processRow(inputRecord._1(), inputRecord._2().iterator());
+    reduceRecordHandler.processRow(inputRecord._1(), inputRecord._2());
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ReduceTran.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ReduceTran.java
index e60dfac..ebd1a18 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ReduceTran.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ReduceTran.java
@@ -22,13 +22,13 @@
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.spark.api.java.JavaPairRDD;
 
-public class ReduceTran implements SparkTran<HiveKey, Iterable<BytesWritable>, HiveKey, BytesWritable> {
+public class ReduceTran implements SparkTran<HiveKey, BytesWritable, HiveKey, BytesWritable> {
   private HiveReduceFunction reduceFunc;
   private String name = "Reduce";
 
   @Override
   public JavaPairRDD<HiveKey, BytesWritable> transform(
-      JavaPairRDD<HiveKey, Iterable<BytesWritable>> input) {
+      JavaPairRDD<HiveKey, BytesWritable> input) {
     return input.mapPartitionsToPair(reduceFunc);
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ShuffleTran.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ShuffleTran.java
index a774395..2aac2b4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ShuffleTran.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/ShuffleTran.java
@@ -23,7 +23,7 @@
 import org.apache.spark.api.java.JavaPairRDD;
 import org.apache.spark.storage.StorageLevel;
 
-public class ShuffleTran implements SparkTran<HiveKey, BytesWritable, HiveKey, Iterable<BytesWritable>> {
+public class ShuffleTran implements SparkTran<HiveKey, BytesWritable, HiveKey, BytesWritable> {
   private final SparkShuffler shuffler;
   private final int numOfPartitions;
   private final boolean toCache;
@@ -42,8 +42,8 @@ public ShuffleTran(SparkPlan sparkPlan, SparkShuffler sf, int n, boolean toCache
   }
 
   @Override
-  public JavaPairRDD<HiveKey, Iterable<BytesWritable>> transform(JavaPairRDD<HiveKey, BytesWritable> input) {
-    JavaPairRDD<HiveKey, Iterable<BytesWritable>> result = shuffler.shuffle(input, numOfPartitions);
+  public JavaPairRDD<HiveKey, BytesWritable> transform(JavaPairRDD<HiveKey, BytesWritable> input) {
+    JavaPairRDD<HiveKey, BytesWritable> result = shuffler.shuffle(input, numOfPartitions);
     if (toCache) {
       sparkPlan.addCachedRDDId(result.id());
       result = result.persist(StorageLevel.MEMORY_AND_DISK());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SortByShuffler.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SortByShuffler.java
index 766813c..9ce187d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SortByShuffler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SortByShuffler.java
@@ -23,10 +23,6 @@
 import org.apache.spark.HashPartitioner;
 import org.apache.spark.Partitioner;
 import org.apache.spark.api.java.JavaPairRDD;
-import org.apache.spark.api.java.function.PairFlatMapFunction;
-import scala.Tuple2;
-
-import java.util.*;
 
 public class SortByShuffler implements SparkShuffler {
 
@@ -40,7 +36,7 @@ public SortByShuffler(boolean totalOrder) {
   }
 
   @Override
-  public JavaPairRDD<HiveKey, Iterable<BytesWritable>> shuffle(
+  public JavaPairRDD<HiveKey, BytesWritable> shuffle(
       JavaPairRDD<HiveKey, BytesWritable> input, int numPartitions) {
     JavaPairRDD<HiveKey, BytesWritable> rdd;
     if (totalOrder) {
@@ -53,7 +49,7 @@ public SortByShuffler(boolean totalOrder) {
       Partitioner partitioner = new HashPartitioner(numPartitions);
       rdd = input.repartitionAndSortWithinPartitions(partitioner);
     }
-    return rdd.mapPartitionsToPair(new ShuffleFunction());
+    return rdd;
   }
 
   @Override
@@ -61,69 +57,4 @@ public String getName() {
     return "SortBy";
   }
 
-  private static class ShuffleFunction implements
-      PairFlatMapFunction<Iterator<Tuple2<HiveKey, BytesWritable>>,
-          HiveKey, Iterable<BytesWritable>> {
-    // make eclipse happy
-    private static final long serialVersionUID = 1L;
-
-    @Override
-    public Iterable<Tuple2<HiveKey, Iterable<BytesWritable>>> call(
-        final Iterator<Tuple2<HiveKey, BytesWritable>> it) throws Exception {
-      // Use input iterator to back returned iterable object.
-      final Iterator<Tuple2<HiveKey, Iterable<BytesWritable>>> resultIt =
-          new Iterator<Tuple2<HiveKey, Iterable<BytesWritable>>>() {
-            HiveKey curKey = null;
-            List<BytesWritable> curValues = new ArrayList<BytesWritable>();
-
-            @Override
-            public boolean hasNext() {
-              return it.hasNext() || curKey != null;
-            }
-
-            @Override
-            public Tuple2<HiveKey, Iterable<BytesWritable>> next() {
-              // TODO: implement this by accumulating rows with the same key into a list.
-              // Note that this list needs to improved to prevent excessive memory usage, but this
-              // can be done in later phase.
-              while (it.hasNext()) {
-                Tuple2<HiveKey, BytesWritable> pair = it.next();
-                if (curKey != null && !curKey.equals(pair._1())) {
-                  HiveKey key = curKey;
-                  List<BytesWritable> values = curValues;
-                  curKey = pair._1();
-                  curValues = new ArrayList<BytesWritable>();
-                  curValues.add(pair._2());
-                  return new Tuple2<HiveKey, Iterable<BytesWritable>>(key, values);
-                }
-                curKey = pair._1();
-                curValues.add(pair._2());
-              }
-              if (curKey == null) {
-                throw new NoSuchElementException();
-              }
-              // if we get here, this should be the last element we have
-              HiveKey key = curKey;
-              curKey = null;
-              return new Tuple2<HiveKey, Iterable<BytesWritable>>(key, curValues);
-            }
-
-            @Override
-            public void remove() {
-              // Not implemented.
-              // throw Unsupported Method Invocation Exception.
-              throw new UnsupportedOperationException();
-            }
-
-          };
-
-      return new Iterable<Tuple2<HiveKey, Iterable<BytesWritable>>>() {
-        @Override
-        public Iterator<Tuple2<HiveKey, Iterable<BytesWritable>>> iterator() {
-          return resultIt;
-        }
-      };
-    }
-  }
-
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlanGenerator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlanGenerator.java
index 510f9b6..8219fb2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlanGenerator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlanGenerator.java
@@ -37,7 +37,6 @@
 import org.apache.hadoop.hive.ql.log.PerfLogger;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.mapred.FileOutputFormat;
-import org.apache.hadoop.mapred.Partitioner;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.ErrorMsg;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkReduceRecordHandler.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkReduceRecordHandler.java
index b7e79d6..80c92a0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkReduceRecordHandler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkReduceRecordHandler.java
@@ -235,9 +235,31 @@ public void init(JobConf job, OutputCollector output, Reporter reporter) throws
     perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.SPARK_INIT_OPERATORS);
   }
 
+  /**
+   * TODO: Instead of creating a dummy iterator per row, we can implement a private method that's
+   * similar to processRow(Object key, Iterator<E> values) but processes one row at a time. Then,
+   * we just call that private method here.
+   */
   @Override
-  public void processRow(Object key, Object value) throws IOException {
-    throw new UnsupportedOperationException("Do not support this method in SparkReduceRecordHandler.");
+  public void processRow(Object key, final Object value) throws IOException {
+    processRow(key, new Iterator<Object>() {
+      boolean done = false;
+      @Override
+      public boolean hasNext() {
+        return !done;
+      }
+
+      @Override
+      public Object next() {
+        done = true;
+        return value;
+      }
+
+      @Override
+      public void remove() {
+        throw new UnsupportedOperationException("Iterator.remove() is not implemented/supported");
+      }
+    });
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkShuffler.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkShuffler.java
index 40e251f..d71d940 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkShuffler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkShuffler.java
@@ -24,7 +24,7 @@
 
 public interface SparkShuffler {
 
-  JavaPairRDD<HiveKey, Iterable<BytesWritable>> shuffle(
+  JavaPairRDD<HiveKey, BytesWritable> shuffle(
       JavaPairRDD<HiveKey, BytesWritable> input, int numPartitions);
 
   public String getName();
diff --git a/ql/src/test/queries/clientpositive/union_top_level.q b/ql/src/test/queries/clientpositive/union_top_level.q
index 946473a..0e5a97c 100644
--- a/ql/src/test/queries/clientpositive/union_top_level.q
+++ b/ql/src/test/queries/clientpositive/union_top_level.q
@@ -15,13 +15,13 @@ union all
 select key, 2 as value from src where key % 3 == 2 limit 3;
 
 explain
-select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 union all
-select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10;
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10;
 
-select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 union all
-select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10;
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10;
 
 -- ctas
 explain
diff --git a/ql/src/test/results/clientpositive/spark/lateral_view_explode2.q.out b/ql/src/test/results/clientpositive/spark/lateral_view_explode2.q.out
index 1ac37a9..8d0b79d 100644
--- a/ql/src/test/results/clientpositive/spark/lateral_view_explode2.q.out
+++ b/ql/src/test/results/clientpositive/spark/lateral_view_explode2.q.out
@@ -93,9 +93,9 @@ POSTHOOK: query: SELECT col1, col2 FROM src LATERAL VIEW explode2(array(1,2,3))
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 #### A masked pattern was here ####
-3	3
 1	1
 2	2
+3	3
 PREHOOK: query: DROP TEMPORARY FUNCTION explode2
 PREHOOK: type: DROPFUNCTION
 PREHOOK: Output: explode2
diff --git a/ql/src/test/results/clientpositive/spark/union_remove_25.q.out b/ql/src/test/results/clientpositive/spark/union_remove_25.q.out
index 79af958..3d136c2 100644
--- a/ql/src/test/results/clientpositive/spark/union_remove_25.q.out
+++ b/ql/src/test/results/clientpositive/spark/union_remove_25.q.out
@@ -416,7 +416,7 @@ Partition Parameters:
 	numFiles            	2                   
 	numRows             	-1                  
 	rawDataSize         	-1                  
-	totalSize           	6826                
+	totalSize           	6812                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
diff --git a/ql/src/test/results/clientpositive/spark/union_top_level.q.out b/ql/src/test/results/clientpositive/spark/union_top_level.q.out
index df7cda0..e7ceab3 100644
--- a/ql/src/test/results/clientpositive/spark/union_top_level.q.out
+++ b/ql/src/test/results/clientpositive/spark/union_top_level.q.out
@@ -1,16 +1,16 @@
 PREHOOK: query: explain
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 PREHOOK: type: QUERY
 POSTHOOK: query: explain
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -137,19 +137,19 @@ STAGE PLANS:
       Processor Tree:
         ListSink
 
-PREHOOK: query: select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+PREHOOK: query: select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 #### A masked pattern was here ####
-POSTHOOK: query: select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+POSTHOOK: query: select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 #### A masked pattern was here ####
@@ -163,14 +163,14 @@ POSTHOOK: Input: default@src
 484	1
 86	2
 PREHOOK: query: explain
-select * from (select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10)a
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 union all
-select * from (select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10)b
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 PREHOOK: type: QUERY
 POSTHOOK: query: explain
-select * from (select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10)a
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 union all
-select * from (select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10)b
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -182,8 +182,8 @@ STAGE PLANS:
       Edges:
         Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 2), Map 4 (PARTITION-LEVEL SORT, 2)
         Reducer 6 <- Map 5 (PARTITION-LEVEL SORT, 2), Map 8 (PARTITION-LEVEL SORT, 2)
-        Reducer 3 <- Reducer 2 (GROUP, 1)
-        Reducer 7 <- Reducer 6 (GROUP, 1)
+        Reducer 3 <- Reducer 2 (SORT, 1)
+        Reducer 7 <- Reducer 6 (SORT, 1)
 #### A masked pattern was here ####
       Vertices:
         Map 1 
@@ -270,19 +270,17 @@ STAGE PLANS:
                   expressions: _col2 (type: string), _col1 (type: string)
                   outputColumnNames: _col0, _col1
                   Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
-                  Limit
-                    Number of rows: 10
-                    Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
-                    Reduce Output Operator
-                      sort order: 
-                      Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
-                      value expressions: _col0 (type: string), _col1 (type: string)
+                  Reduce Output Operator
+                    key expressions: _col0 (type: string)
+                    sort order: +
+                    Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col1 (type: string)
         Reducer 3 
             Reduce Operator Tree:
               Select Operator
-                expressions: VALUE._col0 (type: string), VALUE._col1 (type: string)
+                expressions: KEY.reducesinkkey0 (type: string), VALUE._col0 (type: string)
                 outputColumnNames: _col0, _col1
-                Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
                 Limit
                   Number of rows: 10
                   Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
@@ -307,19 +305,17 @@ STAGE PLANS:
                   expressions: _col2 (type: string), _col1 (type: string)
                   outputColumnNames: _col0, _col1
                   Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
-                  Limit
-                    Number of rows: 10
-                    Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
-                    Reduce Output Operator
-                      sort order: 
-                      Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
-                      value expressions: _col0 (type: string), _col1 (type: string)
+                  Reduce Output Operator
+                    key expressions: _col0 (type: string)
+                    sort order: +
+                    Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col1 (type: string)
         Reducer 7 
             Reduce Operator Tree:
               Select Operator
-                expressions: VALUE._col0 (type: string), VALUE._col1 (type: string)
+                expressions: KEY.reducesinkkey0 (type: string), VALUE._col0 (type: string)
                 outputColumnNames: _col0, _col1
-                Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
                 Limit
                   Number of rows: 10
                   Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
@@ -337,15 +333,15 @@ STAGE PLANS:
       Processor Tree:
         ListSink
 
-PREHOOK: query: select * from (select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10)a
+PREHOOK: query: select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 union all
-select * from (select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10)b
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 #### A masked pattern was here ####
-POSTHOOK: query: select * from (select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10)a
+POSTHOOK: query: select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 union all
-select * from (select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10)b
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 #### A masked pattern was here ####
@@ -359,31 +355,31 @@ POSTHOOK: Input: default@src
 0	val_0
 0	val_0
 0	val_0
-100	val_100
-100	val_100
-100	val_100
-100	val_100
-100	val_100
-100	val_100
-100	val_100
-100	val_100
-104	val_104
-104	val_104
+0	val_0
+0	val_0
+0	val_0
+0	val_0
+0	val_0
+0	val_0
+0	val_0
+0	val_0
+10	val_10
+10	val_10
 PREHOOK: query: explain
 create table union_top as
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 PREHOOK: type: CREATETABLE_AS_SELECT
 POSTHOOK: query: explain
 create table union_top as
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 POSTHOOK: type: CREATETABLE_AS_SELECT
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -528,21 +524,21 @@ STAGE PLANS:
     Stats-Aggr Operator
 
 PREHOOK: query: create table union_top as
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 PREHOOK: type: CREATETABLE_AS_SELECT
 PREHOOK: Input: default@src
 PREHOOK: Output: database:default
 PREHOOK: Output: default@union_top
 POSTHOOK: query: create table union_top as
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 POSTHOOK: type: CREATETABLE_AS_SELECT
 POSTHOOK: Input: default@src
 POSTHOOK: Output: database:default
@@ -572,19 +568,19 @@ POSTHOOK: type: TRUNCATETABLE
 POSTHOOK: Output: default@union_top
 PREHOOK: query: explain
 insert into table union_top
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 PREHOOK: type: QUERY
 POSTHOOK: query: explain
 insert into table union_top
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -723,24 +719,24 @@ STAGE PLANS:
     Stats-Aggr Operator
 
 PREHOOK: query: insert into table union_top
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 PREHOOK: Output: default@union_top
 POSTHOOK: query: insert into table union_top
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 POSTHOOK: Output: default@union_top
-POSTHOOK: Lineage: union_top.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: union_top.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: union_top.value EXPRESSION []
 PREHOOK: query: select * from union_top
 PREHOOK: type: QUERY
@@ -761,19 +757,19 @@ POSTHOOK: Input: default@union_top
 86	2
 PREHOOK: query: explain
 insert overwrite table union_top
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 PREHOOK: type: QUERY
 POSTHOOK: query: explain
 insert overwrite table union_top
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -912,24 +908,24 @@ STAGE PLANS:
     Stats-Aggr Operator
 
 PREHOOK: query: insert overwrite table union_top
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 PREHOOK: Output: default@union_top
 POSTHOOK: query: insert overwrite table union_top
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 POSTHOOK: Output: default@union_top
-POSTHOOK: Lineage: union_top.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: union_top.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: union_top.value EXPRESSION []
 PREHOOK: query: select * from union_top
 PREHOOK: type: QUERY
@@ -950,19 +946,19 @@ POSTHOOK: Input: default@union_top
 86	2
 PREHOOK: query: explain
 create view union_top_view as
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 PREHOOK: type: CREATEVIEW
 POSTHOOK: query: explain
 create view union_top_view as
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 POSTHOOK: type: CREATEVIEW
 STAGE DEPENDENCIES:
   Stage-0 is a root stage
@@ -973,34 +969,34 @@ STAGE PLANS:
         Create View
           or replace: false
           columns: key string, value int
-          expanded text: select `a`.`key`, `a`.`value` from (select `src`.`key`, 0 as `value` from `default`.`src` where `src`.`key` % 3 == 0 limit 3)`a`
+          expanded text: select `src`.`key`, 0 as `value` from `default`.`src` where `src`.`key` % 3 == 0 limit 3
 union all
-select `b`.`key`, `b`.`value` from (select `src`.`key`, 1 as `value` from `default`.`src` where `src`.`key` % 3 == 1 limit 3)`b`
+select `src`.`key`, 1 as `value` from `default`.`src` where `src`.`key` % 3 == 1 limit 3
 union all
-select `c`.`key`, `c`.`value` from (select `src`.`key`, 2 as `value` from `default`.`src` where `src`.`key` % 3 == 2 limit 3)`c`
+select `src`.`key`, 2 as `value` from `default`.`src` where `src`.`key` % 3 == 2 limit 3
           name: default.union_top_view
-          original text: select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+          original text: select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 
 PREHOOK: query: create view union_top_view as
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 PREHOOK: type: CREATEVIEW
 PREHOOK: Input: default@src
 PREHOOK: Output: database:default
 PREHOOK: Output: default@union_top_view
 POSTHOOK: query: create view union_top_view as
-select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
+select key, 0 as value from src where key % 3 == 0 limit 3
 union all
-select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
+select key, 1 as value from src where key % 3 == 1 limit 3
 union all
-select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c
+select key, 2 as value from src where key % 3 == 2 limit 3
 POSTHOOK: type: CREATEVIEW
 POSTHOOK: Input: default@src
 POSTHOOK: Output: database:default
diff --git a/ql/src/test/results/clientpositive/union_top_level.q.out b/ql/src/test/results/clientpositive/union_top_level.q.out
index 3cfbf1a..9560c98 100644
--- a/ql/src/test/results/clientpositive/union_top_level.q.out
+++ b/ql/src/test/results/clientpositive/union_top_level.q.out
@@ -191,14 +191,14 @@ POSTHOOK: Input: default@src
 484	1
 86	2
 PREHOOK: query: explain
-select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 union all
-select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 PREHOOK: type: QUERY
 POSTHOOK: query: explain
-select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 union all
-select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -256,29 +256,27 @@ STAGE PLANS:
             expressions: _col2 (type: string), _col1 (type: string)
             outputColumnNames: _col0, _col1
             Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
-            Limit
-              Number of rows: 10
-              Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
-              File Output Operator
-                compressed: false
-                table:
-                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                    serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
 
   Stage: Stage-2
     Map Reduce
       Map Operator Tree:
           TableScan
             Reduce Output Operator
-              sort order: 
-              Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
-              value expressions: _col0 (type: string), _col1 (type: string)
+              key expressions: _col0 (type: string)
+              sort order: +
+              Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
+              value expressions: _col1 (type: string)
       Reduce Operator Tree:
         Select Operator
-          expressions: VALUE._col0 (type: string), VALUE._col1 (type: string)
+          expressions: KEY.reducesinkkey0 (type: string), VALUE._col0 (type: string)
           outputColumnNames: _col0, _col1
-          Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
+          Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
           Limit
             Number of rows: 10
             Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
@@ -360,29 +358,27 @@ STAGE PLANS:
             expressions: _col2 (type: string), _col1 (type: string)
             outputColumnNames: _col0, _col1
             Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
-            Limit
-              Number of rows: 10
-              Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
-              File Output Operator
-                compressed: false
-                table:
-                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                    serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
 
   Stage: Stage-6
     Map Reduce
       Map Operator Tree:
           TableScan
             Reduce Output Operator
-              sort order: 
-              Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
-              value expressions: _col0 (type: string), _col1 (type: string)
+              key expressions: _col0 (type: string)
+              sort order: +
+              Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
+              value expressions: _col1 (type: string)
       Reduce Operator Tree:
         Select Operator
-          expressions: VALUE._col0 (type: string), VALUE._col1 (type: string)
+          expressions: KEY.reducesinkkey0 (type: string), VALUE._col0 (type: string)
           outputColumnNames: _col0, _col1
-          Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
+          Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
           Limit
             Number of rows: 10
             Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
@@ -399,15 +395,15 @@ STAGE PLANS:
       Processor Tree:
         ListSink
 
-PREHOOK: query: select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10
+PREHOOK: query: select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 union all
-select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 #### A masked pattern was here ####
-POSTHOOK: query: select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10
+POSTHOOK: query: select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 union all
-select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10
+select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) order by k limit 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 #### A masked pattern was here ####
-- 
1.7.9.5

